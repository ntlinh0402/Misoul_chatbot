# app/pdf_processor_langchain.py
import os
import glob
import time
import json
from typing import List, Dict, Any
import traceback
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from config import Config

class MISOULEmbeddings:
    """
    L·ªõp embeddings t∆∞∆°ng th√≠ch v·ªõi LangChain
    
    S·ª≠ d·ª•ng TF-IDF ƒë·ªÉ t·∫°o vector embeddings hi·ªáu qu·∫£
    """
    
    def __init__(self, max_features=5000):
        """
        Kh·ªüi t·∫°o embedding model
        
        Args:
            max_features: S·ªë l∆∞·ª£ng t√≠nh nƒÉng t·ªëi ƒëa
        """
        from sklearn.feature_extraction.text import TfidfVectorizer
        self.vectorizer = TfidfVectorizer(max_features=max_features)
        self.fitted = False
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        T·∫°o embeddings cho danh s√°ch vƒÉn b·∫£n
        
        Args:
            texts: Danh s√°ch c√°c vƒÉn b·∫£n
            
        Returns:
            List[List[float]]: Danh s√°ch c√°c embedding vectors
        """
        # Tr√≠ch xu·∫•t text t·ª´ documents n·∫øu ƒë√≥ l√† ƒë·ªëi t∆∞·ª£ng Document
        if hasattr(texts[0], 'page_content'):
            texts = [doc.page_content for doc in texts]
        
        if not self.fitted:
            self.vectorizer.fit(texts)
            self.fitted = True
        
        embeddings = self.vectorizer.transform(texts).toarray()
        return embeddings.tolist()
    
    def embed_query(self, text: str) -> List[float]:
        """
        T·∫°o embedding cho m·ªôt c√¢u truy v·∫•n
        
        Args:
            text: C√¢u truy v·∫•n
            
        Returns:
            List[float]: Embedding vector
        """
        if not self.fitted:
            # N·∫øu ch∆∞a fit, th√¨ kh√¥ng th·ªÉ embed query
            import numpy as np
            return np.zeros(self.vectorizer.max_features).tolist()
        
        embedding = self.vectorizer.transform([text]).toarray()[0]
        return embedding.tolist()

class PDFProcessor:
    """
    L·ªõp x·ª≠ l√Ω t√†i li·ªáu PDF v√† chuy·ªÉn ƒë·ªïi th√†nh vector embeddings s·ª≠ d·ª•ng LangChain
    """
    
    def __init__(self, pdf_directory=None, vector_db_path=None):
        """
        Kh·ªüi t·∫°o PDFProcessor
        
        Args:
            pdf_directory: Th∆∞ m·ª•c ch·ª©a c√°c file PDF
            vector_db_path: Th∆∞ m·ª•c l∆∞u tr·ªØ vector database
        """
        self.pdf_directory = pdf_directory or Config.PDF_DIRECTORY
        self.vector_db_path = vector_db_path or Config.VECTOR_DB_PATH
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
        if not os.path.exists(self.pdf_directory):
            os.makedirs(self.pdf_directory)
            print(f"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c PDF: {self.pdf_directory}")
            
        if not os.path.exists(self.vector_db_path):
            os.makedirs(self.vector_db_path)
            print(f"‚úÖ ƒê√£ t·∫°o th∆∞ m·ª•c Vector DB: {self.vector_db_path}")
            
        # Kh·ªüi t·∫°o embedding model
        self.embedding_model = MISOULEmbeddings()
    
    @staticmethod
    def check_processing_status():
        """Ki·ªÉm tra tr·∫°ng th√°i ƒë√£ x·ª≠ l√Ω PDF"""
        try:
            status_file = os.path.join(Config.VECTOR_DB_PATH, "process_status.json")
            if not os.path.exists(status_file):
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y file tr·∫°ng th√°i x·ª≠ l√Ω PDF: {status_file}")
                return False
            
            with open(status_file, 'r') as f:
                import json
                status = json.load(f)
                processed = status.get("processed", False)
                print(f"‚úÖ ƒê√£ ƒë·ªçc tr·∫°ng th√°i x·ª≠ l√Ω PDF: {processed}")
                return processed
        except Exception as e:
            print(f"‚ùå Kh√¥ng th·ªÉ ƒë·ªçc tr·∫°ng th√°i x·ª≠ l√Ω PDF: {e}")
            return False

    @staticmethod
    def save_processing_status(processed=True):
        """L∆∞u tr·∫°ng th√°i ƒë√£ x·ª≠ l√Ω PDF"""
        import time
        import json
        
        try:
            status_file = os.path.join(Config.VECTOR_DB_PATH, "process_status.json")
            # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i
            os.makedirs(os.path.dirname(status_file), exist_ok=True)
            
            with open(status_file, 'w') as f:
                json.dump({"processed": processed, "timestamp": time.time()}, f)
            print(f"‚úÖ ƒê√£ l∆∞u tr·∫°ng th√°i x·ª≠ l√Ω PDF: {processed}")
            return True
        except Exception as e:
            print(f"‚ùå Kh√¥ng th·ªÉ l∆∞u tr·∫°ng th√°i x·ª≠ l√Ω PDF: {e}")
            return False
    
    def load_pdf(self, file_path):
        """
        T·∫£i d·ªØ li·ªáu t·ª´ file PDF
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file PDF
            
        Returns:
            list: Danh s√°ch c√°c t√†i li·ªáu
        """
        try:
            loader = PDFPlumberLoader(file_path)
            documents = loader.load()
            print(f"‚úÖ ƒê√£ t·∫£i file PDF {os.path.basename(file_path)}: {len(documents)} trang")
            return documents
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i file PDF {file_path}: {e}")
            return []
    
    def create_chunks(self, documents, chunk_size=1000, chunk_overlap=200):
        """
        Chia t√†i li·ªáu th√†nh c√°c ƒëo·∫°n nh·ªè h∆°n
        
        Args:
            documents: Danh s√°ch c√°c t√†i li·ªáu
            chunk_size: K√≠ch th∆∞·ªõc m·ªói ƒëo·∫°n
            chunk_overlap: S·ªë k√Ω t·ª± ch·ªìng l·∫•p gi·ªØa c√°c ƒëo·∫°n
            
        Returns:
            list: Danh s√°ch c√°c ƒëo·∫°n vƒÉn b·∫£n
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            add_start_index=True
        )
        text_chunks = text_splitter.split_documents(documents)
        print(f"‚úÖ ƒê√£ t·∫°o {len(text_chunks)} ƒëo·∫°n vƒÉn b·∫£n")
        return text_chunks
    
    def add_metadata_to_chunks(self, chunks, file_path):
        """
        Th√™m metadata v√†o c√°c ƒëo·∫°n vƒÉn b·∫£n
        
        Args:
            chunks: Danh s√°ch c√°c ƒëo·∫°n vƒÉn b·∫£n
            file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file PDF
            
        Returns:
            list: Danh s√°ch c√°c ƒëo·∫°n vƒÉn b·∫£n v·ªõi metadata
        """
        filename = os.path.basename(file_path)
        category = self._detect_category(filename)
        
        # Th√™m th√¥ng tin v√†o metadata c·ªßa m·ªói ƒëo·∫°n
        for chunk in chunks:
            chunk.metadata["source"] = filename
            chunk.metadata["file_path"] = file_path
            chunk.metadata["category"] = category
        
        return chunks
    
    def _detect_category(self, filename: str) -> str:
        """
        Ph√°t hi·ªán danh m·ª•c t·ª´ t√™n file
        
        Args:
            filename: T√™n file
            
        Returns:
            str: Danh m·ª•c
        """
        filename_lower = filename.lower()
        
        if any(kw in filename_lower for kw in ["lo_au", "lo-au", "anxiety", "stress", "cang_thang"]):
            return "anxiety"
        elif any(kw in filename_lower for kw in ["tram_cam", "depression", "buon", "tuyetvong"]):
            return "depression"
        elif any(kw in filename_lower for kw in ["cbt", "nhan_thuc", "hanh_vi", "cognitive"]):
            return "cbt_techniques" 
        elif any(kw in filename_lower for kw in ["mindfulness", "thien", "meditation", "chanh_niem"]):
            return "mindfulness"
        elif any(kw in filename_lower for kw in ["khung_hoang", "crisis", "tu_tu", "tu_hai"]):
            return "crisis"
        else:
            return "general_mental_health"
    
    def process_all_pdfs(self):
        """
        X·ª≠ l√Ω t·∫•t c·∫£ c√°c file PDF trong th∆∞ m·ª•c v√† t·∫°o vector database
        
        Returns:
            bool: True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            # Ki·ªÉm tra xem ƒë√£ x·ª≠ l√Ω ch∆∞a
            if PDFProcessor.check_processing_status():
                print("‚úÖ C√°c file PDF ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥. B·ªè qua b∆∞·ªõc x·ª≠ l√Ω.")
                return True
                
            # T√¨m t·∫•t c·∫£ c√°c file PDF
            pdf_files = glob.glob(os.path.join(self.pdf_directory, "*.pdf"))
            
            if not pdf_files:
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y file PDF n√†o trong {self.pdf_directory}")
                return False
            
            print(f"üîç T√¨m th·∫•y {len(pdf_files)} file PDF ƒë·ªÉ x·ª≠ l√Ω")
            
            # L∆∞u tr·ªØ t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn b·∫£n
            all_chunks = []
            
            # X·ª≠ l√Ω t·ª´ng file PDF
            for pdf_file in pdf_files:
                print(f"‚è≥ ƒêang x·ª≠ l√Ω file: {os.path.basename(pdf_file)}")
                
                # T·∫£i PDF
                documents = self.load_pdf(pdf_file)
                
                if not documents:
                    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i {pdf_file}, b·ªè qua file n√†y")
                    continue
                
                # Chia nh·ªè t√†i li·ªáu
                chunks = self.create_chunks(documents)
                
                if not chunks:
                    print(f"‚ö†Ô∏è Kh√¥ng c√≥ ƒëo·∫°n vƒÉn b·∫£n n√†o t·ª´ {pdf_file}, b·ªè qua file n√†y")
                    continue
                
                # Th√™m metadata
                chunks_with_metadata = self.add_metadata_to_chunks(chunks, pdf_file)
                
                # Th√™m v√†o danh s√°ch chung
                all_chunks.extend(chunks_with_metadata)
            
            if not all_chunks:
                print("‚ùå Kh√¥ng c√≥ ƒëo·∫°n vƒÉn b·∫£n n√†o ƒë·ªÉ x·ª≠ l√Ω sau khi ƒë·ªçc t·∫•t c·∫£ c√°c file PDF")
                return False
            
            # T·∫°o vector database v·ªõi FAISS
            print(f"‚è≥ ƒêang t·∫°o FAISS vector database v·ªõi {len(all_chunks)} ƒëo·∫°n vƒÉn b·∫£n...")
            db = FAISS.from_documents(all_chunks, self.embedding_model)
            
            # L∆∞u vector database
            faiss_path = os.path.join(self.vector_db_path, "db_faiss")
            db.save_local(faiss_path)
            print(f"‚úÖ ƒê√£ l∆∞u FAISS vector database v√†o {faiss_path}")
            
            # L∆∞u tr·∫°ng th√°i ƒë√£ x·ª≠ l√Ω
            PDFProcessor.save_processing_status(True)
            
            return True
            
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω PDF: {e}")
            traceback.print_exc()
            return False
    
    @staticmethod
    def load_vector_store():
        """
        T·∫£i vector store t·ª´ ƒëƒ©a
        
        Returns:
            FAISS: FAISS vector store ho·∫∑c None n·∫øu l·ªói
        """
        try:
            faiss_path = os.path.join(Config.VECTOR_DB_PATH, "db_faiss")
            if not os.path.exists(faiss_path):
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y FAISS vector database t·∫°i {faiss_path}")
                return None
            
            # T·∫£i FAISS v·ªõi allow_dangerous_deserialization=True
            embedding_model = MISOULEmbeddings()
            db = FAISS.load_local(faiss_path, embedding_model, allow_dangerous_deserialization=True)
            print(f"‚úÖ ƒê√£ t·∫£i FAISS vector database t·ª´ {faiss_path}")
            
            return db
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i FAISS vector database: {e}")
            traceback.print_exc()
            return None

# H√†m main ƒë·ªÉ ch·∫°y tr·ª±c ti·∫øp
if __name__ == "__main__":
    processor = PDFProcessor()
    success = processor.process_all_pdfs()
    if success:
        print("‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng t·∫•t c·∫£ c√°c file PDF")
    else:
        print("‚ùå C√≥ l·ªói khi x·ª≠ l√Ω c√°c file PDF")